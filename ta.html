<!DOCTYPE html><html><head><title>Temporal Alignment</title><link rel="icon" href="img/head.icon"><link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css"><link rel="stylesheet" href="s/f-zhou.css" type="text/css"></head><body><div id="header"><div id="nav" class="container"><ul><li><a class="item" href="index.html">FEATURED</a></li><li><a class="item" href="pub.html">PUBLICATIONS</a></li><li><a class="item" href="soft.html">SOFTWARES</a></li><li><a class="item" href="bio.html">ABOUT</a></li></ul></div></div><div class="container container_top"><h1>Temporal Alignment of Human Motion</h1><div class="proj_overview"><img src="ta/gtw_overview.png"><p class="main">Temporal alignment of three subjects kicking a                     ball.</p><p class="more">These three sequences are recorded with different sensors                     (top row video, middle row motion capture and bottom row                     accelerometers).</p></div><div class="line_wrap"><div class="line_content"><h2>People</h2></div></div><div class="sec_content"><ul class="li_col"><li>Feng Zhou <span class="place">(CMU)</span></li><li><a href="http://www.cs.cmu.edu/~ftorre" class="level1">Fernando De la Torre</a> <span class="place">(CMU)</span></li></ul></div><div class="line_wrap"><div class="line_content"><h2>Introduction</h2></div></div><div class="sec_content"><p>Temporal alignment of human motion has been a topic of                     recent interest due to its applications in animation,                     tele-rehabilitation and activity recognition among others. This paper                     presents <b>canonical time warping (CTW)</b><sup><a href="#ref1"><span>[</span>1<span>]</span><a href="#ref2"><span>[</span>2<span>]</span><a href="#ref3"><span>[</span>3<span>]</span></a></sup>, an extension                     of dynamic time warping (DTW)<sup><a href="#ref4"><span>[</span>4<span>]</span></a></sup> for temporally aligning                     multi-modal sequences from multiple subjects performing similar                     activities. CTW solves three major drawbacks of existing approaches                     based on DTW:</p><p><ul class="li_feat"><li>CTW provides a feature weighting layer to adapt                                 different modalities (e.g., video and motion capture data);</li><li>CTW extends DTW by allowing a more flexible time warping                                 as combination of monotonic functions;</li><li>Unlike DTW that typically incurs in quadratic cost, CTW                                 has linear complexity.</li></ul></p></div><div class="line_wrap"><div class="line_content"><h2>Code</h2></div></div><div class="sec_content"><p>Available at <a href="ta_code.html" class="level3">here</a>.</p></div><div class="line_wrap"><div class="line_content"><h2>Result</h2></div></div><div class="sec_content"><div class="video_item"><div class="video_main"><iframe src="http://player.vimeo.com/video/45868474?title=0&amp;byline=0&amp;portrait=0&amp;color=ff9933&amp;loop=1" width="500" height="281" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div><div class="video_info"><p>Temporal alignment of three multi-modal sequences by GTW.</p><p>The first two sequences are taken from                             <a href="http://mocap.cs.cmu.edu" class="level3">CMU Motion Capture Database</a> and                             <a href="http://www.wisdom.weizmann.ac.il/~vision/SpaceTimeActions.html" class="level3">Weizmann Action Database</a>.                             The last sequence records the hand movement by an accelerometer. You can                             reproduce the same result using the function <span class="key_word">demoMix.m</span> in the                             code.</p><p>Download the [<a href="ta/jack_ali_gtw.avi" class="level2">Video</a>&nbsp;<span class="pub_info_size">50MB</span>].</p></div></div><div class="video_item"><div class="video_main"><iframe src="http://player.vimeo.com/video/46159740?title=0&amp;byline=0&amp;portrait=0&amp;color=ff9933&amp;loop=1" width="500" height="283" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div><div class="video_info hei_281"><p>Temporal alignment of two motion capture sequences by CTW.</p><p>The motion capture sequences are taken from                             <a href="http://kitchen.cs.cmu.edu" class="level3">CMU Grand Chanllenge Dataset</a>.                             You can reproduce the same result using the function <span class="key_word">demoKit.m</span> in the code.</p><p>Download the [<a href="ta/opening_cabinet.avi" class="level2">Video</a>&nbsp;<span class="pub_info_size">3MB</span>].</p></div></div></div><div class="line_wrap"><div class="line_content"><h2>Publications</h2></div></div><div class="sec_content"><ul class="sec_pub"><li><div class="index" id="ref1">[1]</div><div class="detail"><span class="pub_info_title">Generalized Time Warping for Multi-modal Alignment of Human Motion</span><div class="pub_info_other">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012<br>F. Zhou and <a href="http://www.cs.cmu.edu/~ftorre" class="level1">F. De la Torre</a></div><div class="pub_info_link">[<a class="level2" href="ta/2012_CVPR_GTW.pdf">Paper</a>&nbsp;<span class="pub_info_size">8MB</span>]                                 [<a class="level2" href="ta/gtw_slides.zip">Slides</a>&nbsp;<span class="pub_info_size">14MB</span>]</div></div></li><li><div class="index" id="ref2">[2]</div><div class="detail"><span class="pub_info_title">Canonical Time Warping for Alignment of Human Behavior</span><div class="pub_info_other">Advances in Neural Information Processing Systems (NIPS), 2009<br>F. Zhou and <a href="http://www.cs.cmu.edu/~ftorre" class="level1">F. De la Torre</a></div><div class="pub_info_link">[<a class="level2" href="ta/2009_NIPS_CTW.pdf">Paper</a>&nbsp;<span class="pub_info_size">5MB</span>]</div></div></li><li><div class="index" id="ref3">[3]</div><div class="detail"><span class="pub_info_title">Generalized Canonical Time Warping</span><div class="pub_info_other">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 38(2):279-294, 2016<br>F. Zhou and <a href="http://www.cs.cmu.edu/~ftorre" class="level1">F. De la Torre</a></div><div class="pub_info_link">[<a class="level2" href="ta/2016_PAMI_CTW.pdf">Paper</a>&nbsp;<span class="pub_info_size">18MB</span>]</div></div></li></ul></div><div class="line_wrap"><div class="line_content"><h2>References</h2></div></div><div class="sec_content"><ul class="sec_pub"><li><div class="index" id="ref4">[4]</div><div class="detail"><span class="pub_info_title">Fundamentals of Speech Recognition</span><div class="pub_info_other">Prentice Hall, 1993<br>L. Rabiner and B. Juang</div></div></li></ul></div></div><div class="footer"></div><!-- js --><script src="//code.jquery.com/jquery-1.11.3.min.js"></script><script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script><script type="text/javascript" src="s/retina.js"></script><script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">try {              var pageTracker = _gat._getTracker("UA-5376400-1");              pageTracker._trackPageview();          } catch(err) {}</script></body></html>