<!DOCTYPE html><html><head><title>Facial Events Discovery</title><link rel="icon" href="img/head.ico"><link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css"><link rel="stylesheet" href="s/f-zhou.css" type="text/css"></head><body><div id="header"><div id="nav" class="container"><ul><li><a class="item" href="index.html">FEATURED</a></li><li><a class="item" href="pub.html">PUBLICATIONS</a></li><li><a class="item" href="soft.html">SOFTWARES</a></li><li><a class="item" href="bio.html">ABOUT</a></li></ul></div></div><div class="container container_top"><h1>Facial Events Discovery</h1><div class="proj_overview"><img src="au/overview.png"><p class="main">Selected video frames of unposed facial behavior from three participants.</p><p class="more">Different colors and shapes represent dynamic events           discovered by unsupervised learning: smile (green circle) and lip           compressor (blue hexagons). Dashed lines indicate correspondences           between persons.</p></div><div class="line_wrap"><div class="line_content"><h2>People</h2></div></div><div class="sec_content"><ul class="li_col"><li>Feng Zhou <span class="place">(CMU)</span></li><li><a href="http://www.cs.cmu.edu/~ftorre" class="level1">Fernando De la Torre</a> <span class="place">(CMU)</span></li><li><a href="http://www.pitt.edu/~jeffcohn/" class="level1">Jeffrey F. Cohn</a> <span class="place">(UPitt)</span></li></ul></div><div class="line_wrap"><div class="line_content"><h2>Introduction</h2></div></div><div class="sec_content"><p>Automatic facial image analysis has been a long standing           research problem in computer vision. A key component in           facial image analysis, largely conditioning the success of           subsequent algorithms (e.g., facial expression recognition),           is to define a vocabulary of possible dynamic facial           events. To date, that vocabulary has come from the           anatomically-based <a href="http://en.wikipedia.org/wiki/Facial_Action_Coding_System" class="level3">Facial Action Coding System (FACS)</a> or           more subjective approaches (i.e. emotion-specified           expressions).  The aim of this paper is to discover facial           events directly from video of naturally occurring facial           behavior, without recourse to FACS or other labeling           schemes.  To discover facial events, we propose a novel           temporal clustering algorithm, Aligned Cluster Analysis           (ACA)<sup><a href="#ref2"><span>[</span>2<span>]</span></a> <a href="#ref3"><span>[</span>3<span>]</span></a></sup>,           and a multi-subject correspondence algorithm for matching           expressions.  We use a variety of video sources: posed           facial behavior (Cohn-Kanade database), unscripted facial           behavior (RU-FACS database) and some video in infants.  ACA           achieved moderate intersystem agreement with manual FACS           coding and proved informative as a           visualization/summarization tool.</p></div><div class="line_wrap"><div class="line_content"><h2>Result of a Baby Facial Sequence</h2></div></div><div class="sec_content"><div class="video_item"><div class="video_main"><iframe src="https://player.vimeo.com/video/156173460?title=0&byline=0&portrait=0" width="500" height="375" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><div class="video_info hei_373"><p>This video shows a sequence of baby facial behavior used in the second and third video shown below.</p><p>Download the [<a href="au/baby_original.wmv" class="level2">Video</a>&nbsp;<span class="pub_info_size">3.5MB</span>].</p></div></div><div class="video_item"><div class="video_main"><iframe src="https://player.vimeo.com/video/156175672?title=0&byline=0&portrait=0" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><div class="video_info hei_283"><p>This video shows the facial landmark localization and the geometrical features used by our methods.</p><p>Download the [<a href="au/baby_original.wmv" class="level2">Video</a>&nbsp;<span class="pub_info_size">3.5MB</span>].</p></div></div><div class="video_item"><div class="video_main"><iframe src="https://player.vimeo.com/video/156173889?title=0&byline=0&portrait=0" width="500" height="375" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><div class="video_info hei_373"><p>Our method divides the input baby facial sequence into             five clusters (columns) of temporal segment. Each cluster             (column) corresponds to a particular facial event.</p><p>Download the [<a href="au/baby_original.wmv" class="level2">Video</a>&nbsp;<span class="pub_info_size">3.5MB</span>].</p></div></div><div class="video_item"><div class="video_main"><iframe src="https://player.vimeo.com/video/156174403?title=0&byline=0&portrait=0" width="500" height="375" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><div class="video_info hei_373"><p>Our method offers an efficient video retrieval mechanism.               Given a query video, our method efficiently matches it               to the video segments using a dynamic time warping based similarity (red bar).</p><p>Download the [<a href="au/baby_original.wmv" class="level2">Video</a>&nbsp;<span class="pub_info_size">3.5MB</span>].</p></div></div></div><div class="line_wrap"><div class="line_content"><h2>Result on Benchmark Datasets</h2></div></div><div class="sec_content"><div class="video_item"><div class="video_main"><iframe src="https://player.vimeo.com/video/156175060?title=0&byline=0&portrait=0" width="500" height="375" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><div class="video_info hei_373"><p>Given a synthetic sequence by concatenating several videos from the Cohn-Kanade dataset,               our method can find the coherent temporal clusters of different facial events as well as their low-level embeddings.<p>Download the [<a href="au/baby_original.wmv" class="level2">Video</a>&nbsp;<span class="pub_info_size">3.5MB</span>].</p></div></div><div class="video_item"><div class="video_main"><iframe src="https://player.vimeo.com/video/156175445?title=0&byline=0&portrait=0" width="500" height="375" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><div class="video_info hei_373"><p>Given a synthetic sequence by concatenating several videos from the Cohn-Kanade dataset,               our method can find the coherent temporal clusters of different facial events as well as their low-level embeddings.<p>Download the [<a href="au/baby_original.wmv" class="level2">Video</a>&nbsp;<span class="pub_info_size">3.5MB</span>].</p></div></div></div><div class="line_wrap"><div class="line_content"><h2>Publications</h2></div></div><div class="sec_content"><ul class="sec_pub"><li><div class="index">[1]</div><div class="detail"><span class="pub_info_title">Unsupervised Discovery of Facial Events</span><div class="pub_info_other">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010, Oral<br>F. Zhou, <a href="http://www.cs.cmu.edu/~ftorre" class="level1">F. De la Torre</a> and <a href="http://www.pitt.edu/~jeffcohn/" class="level1">J. F. Cohn</a></div><div class="pub_info_link">[<a class="level2" href="au/2010_CVPR_AU.pdf">Paper</a>&nbsp;<span class="pub_info_size">8MB</span>]                 [<a class="level2" href="au/2010_CVPR_AU_Long.pdf">Longer Version</a>&nbsp;<span class="pub_info_size">12MB</span>]                 [<a class="level2" href="au/au_slides.zip">Slides</a>&nbsp;<span class="pub_info_size">31MB</span>]</div></div></li></ul></div><div class="line_wrap"><div class="line_content"><h2>References</h2></div></div><div class="sec_content"><ul class="sec_pub"><li><div class="index" id="ref2">[2]</div><div class="detail"><span class="pub_info_title">Aligned Cluster Analysis for Temporal Segmentation of Human Motion</span><div class="pub_info_other">International Conference on Automatic Face and Gesture Recognition (FG), 2008<br>F. Zhou, <a href="http://www.cs.cmu.edu/~ftorre" class="level1">F. De la Torre</a> and <a href="http://www.cs.cmu.edu/~jkh" class="level1">J. K. Hodgins</a></div><div class="pub_info_link">[<a class="level2" href="tc/2008_FG_ACA.pdf">Paper</a>&nbsp;<span class="pub_info_size">1MB</span>]</div></div></li><li><div class="index" id="ref3">[3]</div><div class="detail"><span class="pub_info_title">Hierarchical Aligned Cluster Analysis for Temporal Clustering of Human Motion</span><div class="pub_info_other">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 35, no. 3, pp. 582-596, 2013<br>F. Zhou, <a href="http://www.cs.cmu.edu/~ftorre" class="level1">F. De la Torre</a> and <a href="http://www.cs.cmu.edu/~jkh" class="level1">J. K. Hodgins</a></div><div class="pub_info_link">[<a class="level2" href="tc/2013_PAMI_HACA.pdf">Paper</a>&nbsp;<span class="pub_info_size">3MB</span>]</div></div></li></ul></div></div><div class="footer"></div><!-- js --><script src="//code.jquery.com/jquery-1.11.3.min.js"></script><script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script><script type="text/javascript" src="s/retina.js"></script><script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">try {          var pageTracker = _gat._getTracker("UA-5376400-1");          pageTracker._trackPageview();      } catch(err) {}</script></body></html>