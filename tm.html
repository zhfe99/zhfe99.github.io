<!DOCTYPE html><html><head><title>Time Mapping</title><link rel="icon" href="img/head.icon"><link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css"><link rel="stylesheet" href="s/f-zhou.css" type="text/css"></head><body><div id="header"><div id="nav" class="container"><ul><li><a class="item" href="index.html">FEATURED</a></li><li><a class="item" href="pub.html">PUBLICATIONS</a></li><li><a class="item" href="soft.html">SOFTWARES</a></li><li><a class="item" href="bio.html">ABOUT</a></li></ul></div></div><div class="container container_top"><h1>Time Mapping Using Space-Time Saliency</h1><div class="proj_overview"><img src="tm/overview.png"><p class="main">Sampling and filtering using our system. The sampling is denser in     the middle to stretch out a little the more interesting part of     the video.</p><p class="more"></p></div><div class="line_wrap"><div class="line_content"><h2>People</h2></div></div><div class="sec_content"><ul class="li_col"><li>Feng Zhou <span class="place">(CMU)</span></li><li><a href="http://research.microsoft.com/en-us/people/sbkang/" class="level1">Sing Bing Kang</a> <span class="place">(Microsoft Research)</li><li><a href="https://scholar.google.com/citations?user=YAtwLpwAAAAJ&hl=en&oi=ao" class="level1">Michael Cohen</a> <span class="place">(Microsoft Research)</li></ul></div><div class="line_wrap"><div class="line_content"><h2>Introduction</h2></div></div><div class="sec_content"><p>We describe a new approach for generating regular-speed,           low-frame-rate (LFR) video from a high-frame-rate (HFR)           input while preserving the important moments in the           original. We call this <b>time-mapping</b>, a time-based           analogy to high dynamic range to low dynamic range spatial           tone-mapping.  Our approach makes these contributions:</p><p><ul class="li_feat"><li>A robust space-time saliency method for evaluating visual importance;</li><li>A re-timing technique to temporally resample based on frame importance;</li><li>Temporal filters to enhance the rendering of salient motion.</li></ul></p><p>Results of our           space-time saliency method on a benchmark dataset show it is           state-of-the-art. In addition, the benefits of our approach           to HFR-to-LFR time-mapping over more direct methods are           demonstrated in a user study.</p></div><div class="line_wrap"><div class="line_content"><h2>Videos</h2></div></div><div class="sec_content"><div class="video_item"><div class="video_main"><iframe src="//player.vimeo.com/video/97498891?title=0&amp;byline=0&amp;portrait=0" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><div class="video_info hei_279"><p>This video spotlight summarizes the main problem and our contributions.</p><p>Download the [<a href="tm/2014_CVPR_TM_Spotlight.mp4" class="level2">Video</a>&nbsp;<span class="pub_info_size">28MB</span>] .</p></div></div></div><div class="sec_content"><div class="video_item"><div class="video_main"><iframe src="//player.vimeo.com/video/88609187?title=0&amp;byline=0&amp;portrait=0" width="500" height="292" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><div class="video_info hei_290"><p>This video shows the results on the saliency benchmark                dataset and the high-speed video dataset.</p><p>Download the [<a href="tm/2014_CVPR_TM.mp4" class="level2">Video</a>&nbsp;<span class="pub_info_size">24MB</span>]<br>or its [<a href="tm/2014_CVPR_TM.mp4" class="level2">High-Quality Version</a>&nbsp;<span class="pub_info_size">154MB</span>].</p></div></div></div><div class="line_wrap"><div class="line_content"><h2>Code</h2></div></div><div class="sec_content"><p>The space-time saliency implementation is available at <a href="tm_code.html" class="level3">here</a>.</p></div><div class="line_wrap"><div class="line_content"><h2>Publications</h2></div></div><div class="sec_content"><ul class="sec_pub"><li><div class="index" id="ref_tm">[1]</div><div class="detail"><span class="pub_info_title">Time-Mapping Using Space-Time Saliency</span><div class="pub_info_other">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014<br>F. Zhou,                 <a href="http://research.microsoft.com/en-us/people/sbkang/" class="level1">S.-B. Kang</a>                 and <a href="https://scholar.google.com/citations?user=YAtwLpwAAAAJ&hl=en&oi=ao" class="level1">M. Cohen</a></div><div class="pub_info_link">[<a href="tm/2014_CVPR_TM.pdf" class="level2">Paper</a>&nbsp;<span class="pub_info_size">7MB</span>]                 [<a href="tm/2014_CVPR_TM_High.pdf" class="level2">High-Resolution Version</a>&nbsp;<span class="pub_info_size">55MB</span>]                 [<a href="tm/2014_CVPR_TM_Sup.pdf" class="level2">Appendix</a>&nbsp;<span class="pub_info_size">2MB</span>]                 [<a class="level2" href="tm/2014_CVPR_TM.pptx">Slides</a>&nbsp;<span class="pub_info_size">119MB</span>]</div></div></li></ul></div><div class="line_wrap"><div class="line_content"><h2>Acknowledgements</h2></div></div><div class="sec_content"><p>Our initial investigation with <a href="http://www.inf.ufrgs.br/~eslgastal/" class="level1">Eduardo Gastal</a> on processing high-speed videos helped to provide focus on our time-mapping project. We would also like to thank him and <a href="http://www.linkedin.com/pub/patrick-meegan/12/140/a9" class="level1">Patrick Meegan</a> for capturing the high-speed video clips used in this paper.</p></div></div><div class="footer"></div><!-- js --><script src="//code.jquery.com/jquery-1.11.3.min.js"></script><script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script><script type="text/javascript" src="s/retina.js"></script><script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");       document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">try {       var pageTracker = _gat._getTracker("UA-5376400-1");       pageTracker._trackPageview();       } catch(err) {}</script></body></html>